{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b7d20",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "In our [last notebook](../03_ArtificialNeurons/Supplement_MultivariateActivationFunctions.ipynb), we learned how multiple inputs to a single activation function can influence the activation function value. Given a particular set of inputs, we interpreted the function's output to be the degree of confidence in a positive or negative decision; for instance, a output of `0.80` corresponds to an 80% confidence that a positive decision is the correct one for the provided data (and conversely, a 20% confidence that a negative decision is the correct one).\n",
    "\n",
    "In this notebook, we create a new mathematical model by composing these activation functions into a network of activation functions wherein the output of several activation functions serves as weighted input into another activation function. Such models are known as _neural networks_, and are extraordinarily well-equipped to learn an extremely wide variety of classification and decision functions.\n",
    "\n",
    "### Software Prerequisites\n",
    "\n",
    "The following Python libraries are prerequisites to run this notebook; simply run the following code block to install them. They are also listed in the `requirements.txt` file in the root of this notebook's [GitHub repository](https://github.com/uccs-math-clinic/mc-workshops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79290e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 \\\n",
    "             numpy==1.21.5 \\\n",
    "             torch==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20198ac",
   "metadata": {},
   "source": [
    "The Python kernel must be restarted after running the above code block for the first time within a particular virtual environment. This may be accomplished by navigating to `Kernel -> Restart` in the menu bar.\n",
    "\n",
    "With our package dependencies installed, we can run the following [boilerplate code](https://en.wikipedia.org/wiki/Boilerplate_code) in order to import the packages needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae5321",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8701da3",
   "metadata": {},
   "source": [
    "## Introducing our Model\n",
    "\n",
    "Earlier, we claimed that neural networks are well-suited to learn a wide variety of classification and decision functions. Theoretically, neural networks act as [universal function approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem); that is, given sufficient training data and neurons within the middle (hidden) layers, neural networks are capable of approximating arbitrary continuous functions defined on compact subsets of $\\mathbb{R}^n$.\n",
    "\n",
    "In practice, there exist pragmatic computational constraints upon the structure and size of neural networks which are feasible to work with. A neural network's efficacy in classification and training efficiency depend highly on both the quality of training data (i.e., how well the data actually represents the phenomenon we wish to model) and the structure of the underlying neural network. Oftentimes, it is only the latter of these over which we have any control; in fact, choosing an appropriate neural network structure for a particular data set often comprises much of the work behind learning a useful data model.\n",
    "\n",
    "Recall that neural networks consist of weighted compositions of activation functions. In particular, a set of activation functions (or neurons) whose output serves as weighted input into another activation function is said to comprise a _layer_ of the neural network. In this notebook, we use the now-familiar logistic function as our activation function of choice, though of course [many](https://en.wikipedia.org/wiki/ReLU) [others](https://en.wikipedia.org/wiki/Gaussian_function) [exist](https://en.wikipedia.org/wiki/Heaviside_step_function).\n",
    "\n",
    "### Notation\n",
    "Suppose that we wish to calculate the input value for the second layer of a neural network which takes three distinct values (or input neurons) as input. Each of these input values has associated with it a weight which is unique with respect to the destination neuron. We observed in the previous notebook that keeping track of even two inputs along with their correspondings weights, biases, and gradient quickly became tiresome. As we consider networks which may contain hundreds or even millions of neurons in a single layer, it becomes apparent that we must stretch our intuition somewhat to adopt a new notation which we can use to model complex data sets with many interconnected layers. We achieve this by way of matrix computation. For example, we calculate all input terms for each of the two neurons in the second layer simultaneously via the following matrix calculation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{11} \\\\\n",
    "b_{21}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we break this down, we see that the result is a $2 \\times 1$ column matrix whose entries correspond to the input values for each of the two neurons in the second layer. The model inputs (the vector $\\textbf{x}$) are a $3 \\times 1$ column vector wherein each entry corresponds to the activation function value for each of the three _input_ neurons. Moreover, we see that the weight matrix contains a single unique value for each activation function value and input value pair; the element $w_23$ represents the weight applied to the _third_ neuron which serves as an input component for the _second_ neuron of the next layer - that is, the subscript indices are to be read _backward_ when interpreting what role a particular weight plays in the network. If one were to encounter the notation for a particular weight element prior to learning the matrix representation for these calculations, this indexing scheme would seem quite backward indeed!\n",
    "\n",
    "To complete our development of neural network notation, we must account for the fact that the above matrix only represents a single layer in the network. The standard convention for this is to indicate the layer into which inputs are fed as a superscript; in our example network, the $w_23$ would therefore be written $w_23^2$, as the weight is applied to input _into_ the second layer of the network. At this point, we have quite a few numerical subscripts and superscripts to think about. Internalizing this notation comes with time, but for the sake of simplifying matters somewhat as we develop this intuition, we deviate slightly from the standard notation by denoting neural network layers by their corresponding uppercase letter in the alphabet. For example, we denote the input/first layer by $A$; for a three-layer network, the final layer is denoted $C$, etc. In practice, these layers are denoted by natural numbers, and so this deviation becomes the standard (and hence extensible to arbitrary network depths) simply by exchanging alphabetical characters for their corresponding numerical position in the alphabet.\n",
    "\n",
    "With our motivations firmly established, we denote  the weights, biases, and activation function values for a neural network accordingly:\n",
    "\n",
    "- The weight applied to the $k^{th}$ output neuron from layer $A$ which contributes to the input for the $j^{th}$ neuron in layer $B$ is denoted $w_{j \\leftarrow k}^B$. Furthermore, we denote the set of all weights which contribute to the input for the $j^{th}$ neuron in layer $B$ by the vector $\\textbf{w}_j^B$, and we denote the set of all weights for a given layer by $\\textbf{w}^B$.\n",
    "\n",
    "- The bias added to the input for the $j^{th}$ neuron in layer $B$ is denoted $b_{j}^B$, and we denote the set of all biases for a given layer by $\\textbf{b}^B$.\n",
    "\n",
    "- The activation function value of the $k^{th}$ output neuron from layer $A$ which contributes to the input for the $j^{th}$ neuron in layer $B$ is denoted $x_{j \\leftarrow k}^B$. Furthermore, we denote the set of all activation function values which contribute to the input for the $j^{th}$ neuron in layer $B$ by the vector $\\textbf{x}_j^B$, and we denote the set of all activation function values for a given layer by $\\textbf{x}^B$.\n",
    "\n",
    "Lastly, we denote the weighted sum of output values and biases from layer $A$ into the $j^{th}$ neuron in layer $B$ by $\\textbf{z}_j^B$ and is defined by $\\textbf{z}_j^B = \\textbf{w}_j^B \\cdot \\textbf{x}_j^B + \\textbf{b}_j^B$, where $\\textbf{w}_j^B \\cdot \\textbf{x}_j^B$ represents the [dot product](https://en.wikipedia.org/wiki/Dot_product) between $\\textbf{w}_j^B$ and $\\textbf{x}_j^B$. In similar fashion to weights and biases, the set of all such inputs into the activation functions which comprise layer $B$ is denoted $\\textbf{z}^B$. We will utilize this particular notation heavily as we investigate the gradient of the loss function used to train a neural network.\n",
    "\n",
    "It is worth spending time becoming comfortable with this notation, as we will use it extensively.\n",
    "\n",
    "### One Hidden Layer\n",
    "We begin by considering a very simple neural network to approximate $f(x) = sin(x)$ over the interval $[0, 2\\pi]$. Our neural network will consist of a single input layer (or activation function) connected to a three-neuron hidden layer which is in turn connected to a single-neuron output layer. We interpret the output of this last layer to be the predicted $y$-value for the model function. Qualitatively, our network looks like this:\n",
    "\n",
    "![](nn.svg)\n",
    "\n",
    "With the notation introduced earlier, our network $N(x)$ take the following form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    N(\\textbf{x}, \\textbf{w}, \\textbf{b}) &= \\sigma(\\textbf{w}^C \\cdot \\textbf{x}^C + \\textbf{b}^C) \\\\\n",
    "                                          &= \\sigma(\\textbf{z}^C)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Just as with every model which we've worked with thus far, in order to _train_ such a network, we require a loss function which we can minimize with respect to the model's weight and bias terms. This requires a gradient calculation for every weight and bias term in the network. As neural networks scale in complexity, it is apparent that training a neural network consists largely of calculating these gradient terms and using those results to adjust optimal weights and biases. In contrast to the methodology used in our development of the notation used to express a neural network, we will derive the gradient terms explicitly for our single-hidden-layer network and then construct a generalized form for calculating the network gradient from there for generalized networks.\n",
    "\n",
    "Let's examine the weight applied to the output of the first neuron in the first layer to second neuron in the second layer which by our above notation is denoted $w_{2 \\leftarrow 1}^B$. In order to adjust this weight via gradient descent, we need to calculate the impact that this weight has on the total loss function. In mathematical terms, this amounts to calculating the partial derivative of the loss function with respect to this weight, $\\frac{\\partial L}{\\partial w_{2 \\leftarrow 1}^B}$. With our usual MSE function, the chain rule recursively yields this value over several steps:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_{2 \\leftarrow 1}^B} &= \\left(\\frac{1}{n} \\sum\\limits_{i=0}^{n}{y_i - N(x_i)}\\right) \\cdot \\frac{\\partial N}{\\partial w_{2 \\leftarrow 1}^B} \\\\\n",
    "    \\frac{\\partial N}{\\partial w_{2 \\leftarrow 1}^B} &= \\sigma(z_1^C) \\cdot (1 - \\sigma(z_1^C)) \\cdot \\frac{\\partial z_1^C}{\\partial w_{2 \\leftarrow 1}^B} \\\\\n",
    "    \\frac{\\partial z_1^C}{\\partial w_{2 \\leftarrow 1}^B} &= w_{1 \\leftarrow 2}^C \\cdot \\sigma(z_2^B) \\cdot (1 - \\sigma(z_2^B)) \\cdot \\frac{\\partial z_2^B}{\\partial w_{2 \\leftarrow 1}^B} + b_2^C \\\\\n",
    "    \\frac{\\partial z_2^B}{\\partial w_{2 \\leftarrow 1}^B} &= x_1^B\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Don't worry if this is a lot to unpack - understanding this chain takes some time (and probably some pen and paper!) to understand, and we fortunately really only need to make a couple key observations about what's going on here in order to come up with a way to calculate each desired gradient term. The first observation we make is that each step of the partial derivative computation above moves us backward in the network; that is, we start with the overall loss term (which we've already calculated), and derive the partial derivative in terms of the incremental change in the prior layer. In doing so, we effectively _propagate_ the changes which need to be made in response to the calculated error _backward_ through the network. This process is called _backpropagation_, and is often one of the trickier aspects of neural networks to master. Fortunately for us, we know that backpropagation is really just adjusting weights and biases (parameters) by way of the Chain Rule applied to a composition of sigmoid functions.\n",
    "\n",
    "The second important observation to make is that by the time we calculate the gradient of the loss function with respect to a particular weight in the _first_ layer of the neural network, we've already performed a series of computations which can be used to calculate the required changes to _all other weights in the network_. We therefore only need a single backward pass through the network in order to compute the gradient term for every single weight and bias in the network! The details of implementing this in code lie somewhat outside the scope of this notebook; we refer the interested reader to Michael Nielson's [excellent in-depth development](http://neuralnetworksanddeeplearning.com/chap2.html) of the backpropagation algorithm and reference implementation. In practice, nearly every machine learning library provides an efficient implementation of backpropagation.\n",
    "\n",
    "### Learning the Sine Function\n",
    "\n",
    "We now turn our attention toward solving the problem we initially posed, and enlist the help of an extremely popular machine learning library called [PyTorch](https://pytorch.org/) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf713a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_points = 64\n",
    "\n",
    "# Let's first generate our learning target. We'll add some noise to it so that we're\n",
    "# modeling a data set which is a bit imperfect. The y-values represent the function\n",
    "# that we're trying to learn. Since we're generating this data ourselves, we know that\n",
    "# it should be sinusoidal, but the neural network doesn't know that - it'll figure it\n",
    "# out over the course of training.\n",
    "#\n",
    "x_vals = np.linspace(0, 2 * np.pi, data_points).reshape(-1, 1)  # .reshape(-1, 1) turns x_vals into a column vector.\n",
    "y_vals = np.sin(x_vals) + np.random.uniform(-0.3, 0.3, x_vals.shape)\n",
    "\n",
    "train_data_fig, train_data_ax = plt.subplots()\n",
    "\n",
    "train_data_ax.set_title('Training Data')\n",
    "train_data_ax.scatter(x_vals, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5103a54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our target data set now defined, we are ready to train our very first neural network with PyTorch. The ideas and concepts we've developed up to this point have been leading up to this, and we've seen some of the complexity behind many of the seemingly simple calls in the proceeding code blocks. In our very first workshop, we proposed that it might be useful to extract commonly-used algorithms into a library - the proceeding code block demonstrates just how powerful and useful well-writen libraries can be. Adjusting the values in the call to the `visualize_neural_network` function (and hence `train_nn`) can be particularly illuminating in understanding how each parameter affects network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9779e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_nn(x_np, y_np, hidden_layer_size, activation_function, iterations, learning_rate=0.01, device='cpu'):\n",
    "    # w1 and b1 are the weights and biases going into the hidden layer\n",
    "    # We'll initialize these to random numbers.\n",
    "    #\n",
    "    w1 = torch.randn(hidden_layer_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "    b1 = torch.randn(hidden_layer_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "    # w2 represents the biases applied to the output of the hidden layer.\n",
    "    # We'll initialize these to random numbers.\n",
    "    #\n",
    "    w2 = torch.randn(hidden_layer_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "    optimizer = optim.SGD([w1, b1, w2], lr=learning_rate)\n",
    "\n",
    "    # Convert numpy arrays to native tensors, which are sent to the hardware used to train the\n",
    "    # neural network (GPUs are often used to speed this part up).\n",
    "    #\n",
    "    x = torch.from_numpy(x_np).float().to(device)\n",
    "    y = torch.from_numpy(y_np).float().to(device)\n",
    "\n",
    "    train_err_vals = []\n",
    "\n",
    "    for i in range(0, iterations):  # Run at least once no matter what\n",
    "        # Calculate predicted values for each x value, weight and bias.\n",
    "        #\n",
    "        y_predicted = (activation_function(x * w1 + b1) * w2).sum(1)\n",
    "\n",
    "        # This is our standard Mean Square Error loss for training data, which we've implemented e\n",
    "        # explicitly a few times by this point.\n",
    "        #\n",
    "        train_err = ((y_predicted - y[:,0])**2).mean()\n",
    "\n",
    "        # This single line executes backpropagation throughout the entire network - this is\n",
    "        # where libraries such as PyTorch really shine!\n",
    "        #\n",
    "        train_err.backward()\n",
    "\n",
    "        # Carry out one optimization step with the provided optimizer\n",
    "        #\n",
    "        optimizer.step()\n",
    "\n",
    "        # Reset gradients to zero - see this Stack Overflow question and answer for more details\n",
    "        # as to why we do this: https://stackoverflow.com/a/48009142\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # We'll print a message every 1000 iterations so that we have some idea of where model\n",
    "        # training is at.\n",
    "        #\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print('Completed {} training iterations.'.format(i + 1))\n",
    "\n",
    "        # Save error for this iteration by appending its value to the err_vals array.\n",
    "        #\n",
    "        # Note that the call to .item() fetches the value from a single-valued PyTorch tensor\n",
    "        # (of which this is one).\n",
    "        #\n",
    "        train_err_vals.append(train_err.item())\n",
    "\n",
    "    # Get predicted values as a numpy array for plotting, analysis, etc.\n",
    "    # These values represent the predictions made by our neural network.\n",
    "    #\n",
    "    return y_predicted.detach().cpu().numpy(), train_err_vals\n",
    "\n",
    "\n",
    "def visualize_neural_network(x, y, hidden_layer_size, activation_function, iterations, learning_rate):\n",
    "    N_x, train_err_vals = train_nn(\n",
    "        x,\n",
    "        y,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        activation_function=activation_function,\n",
    "        iterations=iterations,\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "\n",
    "    # Create some plots with nice titles and layout.\n",
    "    #\n",
    "    fig, [err_ax, ax]= plt.subplots(2, 1)\n",
    "\n",
    "    ax.set_title('Neural Network Prediction')\n",
    "    err_ax.set_title('Error (MSE)')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Plot training data and predictions\n",
    "    #\n",
    "    ax.scatter(x, y)\n",
    "    ax.plot(x, N_x, color='orange')\n",
    "\n",
    "    # Plot training error\n",
    "    #\n",
    "    err_ax.plot(np.arange(0, len(train_err_vals)), train_err_vals, color='red')\n",
    "\n",
    "    return fig, ax, err_ax\n",
    "\n",
    "nn_fig, nn_ax, nn_err_ax = visualize_neural_network(\n",
    "    x_vals,\n",
    "    y_vals,\n",
    "    hidden_layer_size=3,\n",
    "    activation_function=nn.Sigmoid(),\n",
    "    iterations=20 * 1000,\n",
    "    learning_rate=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9bf18",
   "metadata": {},
   "source": [
    "## Can a neural network be _too_ good?\n",
    "\n",
    "Recall that given enough training data and enough neurons in the hidden layer, neural networks are always theoretically capable of approximating the function which generated that data to an arbitrary degree of precision. Contrary to what one might initially think, more precision is not always better - because neural networks are so well-suited to matching data sets, it is easy for neural networks to incorrectly model the _actual data provided_ instead of the _phenomenon which generated the data_. When this occurs, a neural network is said to have _overfit_ the data set - this generally means that the learned model does not generalize well to similar data sets. One way to mitigate this effect is to split the available data set into _training data_ and _validation data_. From there, **only** the training data is used to learn network weights and biases, and the validation data is used as a measure of how well the network has learned to model the underlying function more so than the data set itself. Since the validation data is never used to train the network, we'd like for the validation loss to be as low as possible; a low validation loss indicates that a particular network is likely to generalize well to data sets similar to that which the model was trained on.\n",
    "\n",
    "This notebook's exercise is to calculate the MSE value for a validation set at each training iteration, plot that in the same view as the training loss, and use that information to decide at what point (if any) the model starts to overfit the data set. We'll generate the validation set and pass the provided $x$-values as the `x_np_validation` parameter in the `train_nn_with_validation` function below.\n",
    "\n",
    "_Hint:_ Take a look at `train_err_vals` to see how those are calculated. Calculating the validation error (loss) values will probably be very similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67efee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_with_validation(\n",
    "    x_np,\n",
    "    y_np,\n",
    "    x_np_validation,\n",
    "    y_np_validation,\n",
    "    hidden_layer_size,\n",
    "    activation_function,\n",
    "    iterations,\n",
    "    learning_rate=0.01, device='cpu'\n",
    "):\n",
    "    # Randomly initialize weights and biases\n",
    "    #\n",
    "    w1 = torch.randn(hidden_layer_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "    b1 = torch.randn(hidden_layer_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "    w2 = torch.randn(hidden_layer_size, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "    # Choose optimizer\n",
    "    #\n",
    "    optimizer = optim.SGD([w1, b1, w2], lr=learning_rate)\n",
    "\n",
    "    # Transform numpy data sets into PyTorch data sets.\n",
    "    #\n",
    "    x = torch.from_numpy(x_np).float().to(device)\n",
    "    y = torch.from_numpy(y_np).float().to(device)\n",
    "\n",
    "    x_validation = torch.from_numpy(x_np_validation).float().to(device)\n",
    "    y_validation = torch.from_numpy(y_np_validation).float().to(device)\n",
    "\n",
    "    train_err_vals = []\n",
    "\n",
    "    validation_err_vals = []\n",
    "\n",
    "    for i in range(0, iterations):  # Run at least once no matter what\n",
    "        y_predicted = (activation_function(x * w1 + b1) * w2).sum(1)\n",
    "        y_validation_predictions = (activation_function(x_validation * w1 + b1) * w2).sum(1)\n",
    "\n",
    "        train_err = ((y_predicted - y[:,0])**2).mean()\n",
    "\n",
    "        validation_err = ((y_validation_predictions - y_validation[:,0])**2).mean()\n",
    "\n",
    "        train_err.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print('Completed {} training iterations (epochs).'.format(i + 1))\n",
    "\n",
    "\n",
    "        train_err_vals.append(train_err.item())\n",
    "\n",
    "        validation_err_vals.append(validation_err.item())\n",
    "\n",
    "    return y_predicted.detach().cpu().numpy(), train_err_vals, validation_err_vals\n",
    "\n",
    "\n",
    "def visualize_neural_network_with_validation(\n",
    "    x,\n",
    "    y,\n",
    "    x_validation_set,\n",
    "    y_validation_set,\n",
    "    hidden_layer_size,\n",
    "    activation_function,\n",
    "    iterations,\n",
    "    learning_rate\n",
    "):\n",
    "    N_x, train_err_vals, validation_err_vals = train_nn_with_validation(\n",
    "        x,\n",
    "        y,\n",
    "        x_validation_set,\n",
    "        y_validation_set,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        activation_function=activation_function,\n",
    "        iterations=iterations,\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "\n",
    "    # Create some plots with nice titles and layout.\n",
    "    #\n",
    "    fig, [err_ax, ax]= plt.subplots(2, 1)\n",
    "\n",
    "    ax.set_title('Neural Network Prediction')\n",
    "    err_ax.set_title('Error (MSE)')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Plot training data and predictions\n",
    "    #\n",
    "    ax.scatter(x, y)\n",
    "    ax.plot(x, N_x, color='orange')\n",
    "\n",
    "    # Plot training error\n",
    "    #\n",
    "    err_ax.plot(np.arange(0, len(train_err_vals)), train_err_vals, color='red')\n",
    "    err_ax.plot(np.arange(0, len(validation_err_vals)), validation_err_vals, color='blue')\n",
    "\n",
    "    return fig, ax, err_ax\n",
    "\n",
    "# Create a set of 16 validation data points. Usually, this should comprise ~15% or so of your input data set.\n",
    "#\n",
    "x_validation_vals = np.linspace(0, 2 * np.pi, 16).reshape(-1, 1)\n",
    "y_validation_vals = np.sin(x_validation_vals) + np.random.uniform(-0.1, 0.5, x_validation_vals.shape)\n",
    "\n",
    "nn_fig, nn_ax, nn_err_ax = visualize_neural_network_with_validation(\n",
    "    x_vals,\n",
    "    y_vals,\n",
    "    x_validation_vals,\n",
    "    y_validation_vals,\n",
    "    hidden_layer_size=3,\n",
    "    activation_function=nn.Sigmoid(),\n",
    "    iterations=20 * 1000,\n",
    "    learning_rate=0.1,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}