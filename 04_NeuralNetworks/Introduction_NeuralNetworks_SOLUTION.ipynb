{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b7d20",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "In our [last notebook](../03_ArtificialNeurons/Supplement_MultivariateActivationFunctions.ipynb), we learned how multiple inputs to a single activation function can influence the activation function value. Given a particular set of inputs, we interpreted the function's output to be the degree of confidence in a positive or negative decision; for instance, a output of `0.80` corresponds to an 80% confidence that a positive decision is the correct one for the provided data (and conversely, a 20% confidence that a negative decision is the correct one).\n",
    "\n",
    "In this notebook, we create a new mathematical model by composing these activation functions into a network of activation functions wherein the output of several activation functions serves as weighted input into another activation function. Such models are known as _neural networks_, and are extraordinarily well-equipped to learn an extremely wide variety of classification and decision functions.\n",
    "\n",
    "### Software Prerequisites\n",
    "\n",
    "The following Python libraries are prerequisites to run this notebook; simply run the following code block to install them. They are also listed in the `requirements.txt` file in the root of this notebook's [GitHub repository](https://github.com/uccs-math-clinic/mc-workshops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edab814",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 \\\n",
    "             numpy==1.21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e807e1",
   "metadata": {},
   "source": [
    "The Python kernel must be restarted after running the above code block for the first time within a particular virtual environment. This may be accomplished by navigating to `Kernel -> Restart` in the menu bar.\n",
    "\n",
    "With our package dependencies installed, we can run the following [boilerplate code](https://en.wikipedia.org/wiki/Boilerplate_code) in order to import the packages needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f48ad",
   "metadata": {},
   "source": [
    "## Introducing our Model\n",
    "\n",
    "Earlier, we claimed that neural networks are well-suited to learn a wide variety of classification and decision functions. Theoretically, neural networks act as [universal function approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem); that is, given sufficient training data and neurons within the middle (hidden) layers, neural networks are capable of approximating arbitrary continuous functions defined on compact subsets of $\\mathbb{R}^n$.\n",
    "\n",
    "In practice, there exist pragmatic computational constraints upon the structure and size of neural networks which are feasible to work with. A neural network's efficacy in classification and training efficiency depend highly on both the quality of training data (i.e., how well the data actually represents the phenomenon we wish to model) and the structure of the underlying neural network. Oftentimes, it is only the latter of these over which we have any control; in fact, choosing an appropriate neural network structure for a particular data set often comprises much of the work behind learning a useful data model.\n",
    "\n",
    "Recall that neural networks consist of weighted compositions of activation functions. In particular, a set of activation functions (or neurons) whose output serves as weighted input into another activation function is said to comprise a _layer_ of the neural network. In this notebook, we use the now-familiar logistic function as our activation function of choice, though of course [many](https://en.wikipedia.org/wiki/ReLU) [others](https://en.wikipedia.org/wiki/Gaussian_function) [exist](https://en.wikipedia.org/wiki/Heaviside_step_function).\n",
    "\n",
    "### Notation\n",
    "Suppose that we wish to calculate the input value for the second layer of a neural network which takes three distinct values (or input neurons) as input. Each of these input values has associated with it a weight which is unique with respect to the destination neuron. We observed in the previous notebook that keeping track of even two inputs along with their correspondings weights, biases, and gradient quickly became tiresome. As we consider networks which may contain hundreds or even millions of neurons in a single layer, it becomes apparent that we must stretch our intuition somewhat to adopt a new notation which we can use to model complex data sets with many interconnected layers. We achieve this by way of matrix computation. For example, we calculate all input terms (or activation values) for each of the two neurons in the second layer simultaneously via the following matrix calculation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{11} \\\\\n",
    "b_{21}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we break this down, we see that the result is a $2 \\times 1$ column matrix whose entries correspond to the input values for each of the two neurons in the second layer. The model inputs (the vector $\\textbf{x}$) are a $3 \\times 1$ column vector wherein each entry corresponds to the activation function value for each of the three _input_ neurons. Moreover, we see that the weight matrix contains a single unique value for each activation function value and input value pair; the element $w_23$ represents the weight applied to the _third_ neuron which serves as an input component for the _second_ neuron of the next layer - that is, the subscript indices are to be read _backward_ when interpreting what role a particular weight plays in the network. If one were to encounter the notation for a particular weight element prior to learning the matrix representation for these calculations, this indexing scheme would seem quite backward indeed!\n",
    "\n",
    "To complete our development of neural network notation, we must account for the fact that the above matrix only represents a single layer in the network. The standard convention for this is to indicate the layer into which inputs are fed as a superscript; in our example network, the $w_23$ would therefore be written $w_23^2$, as the weight is applied to input _into_ the second layer of the network. At this point, we have quite a few numerical subscripts and superscripts to think about. Internalizing this notation comes with time, but for the sake of simplifying matters somewhat as we develop this intuition, we deviate slightly from the standard notation by denoting neural network layers by their corresponding uppercase letter in the alphabet. For example, we denote the input/first layer by $A$; for a three-layer network, the final layer is denoted $C$, etc. In practice, these layers are denoted by natural numbers, and so this deviation becomes the standard (and hence extensible to arbitrary network depths) simply by exchanging alphabetical characters for their corresponding numerical position in the alphabet.\n",
    "\n",
    "With our motivations firmly established, we denote  the weights, biases, and activation function values for a neural network accordingly:\n",
    "\n",
    "- The weight applied to the $k^{th}$ output neuron from layer $A$ which contributes to the input for the $j^{th}$ neuron in layer $B$ is denoted $w_{j \\leftarrow k}^B$. Furthermore, we denote the set of all weights which contribute to the input for the $j^{th}$ neuron in layer $B$ by the vector $\\textbf{w}_j^B$.\n",
    "\n",
    "- The bias added to the input for the $j^{th}$ neuron in layer $B$ is denoted $b_{j}^B$.\n",
    "\n",
    "- The activation function value of the $k^{th}$ output neuron from layer $A$ which contributes to the input for the $j^{th}$ neuron in layer $B$ is denoted $x_{j \\leftarrow k}^B$. Furthermore, we denote the set of all activation function values which contribute to the input for the $j^{th}$ neuron in layer $B$ by the vector $\\textbf{x}_j^B$.\n",
    "\n",
    "It is worth spending time becoming comfortable with this notation, as we will use it extensively.\n",
    "\n",
    "### One Hidden Layer\n",
    "We begin by training a very simple neural network to approximate $f(x) = sin(x)$ over the interval $[0, 2\\pi]$. Our neural network will consist of a single input layer (or activation function) connected to a two-neuron hidden layer which is in turn connected to a single-neuron output layer. We interpret the output of this last layer to be the predicted $y$-value for the model function. Qualitatively, our network looks like this:\n",
    "\n",
    "![](./nn.svg)\n",
    "\n",
    "With the notation introduced earlier, our network $N(x)$ becomes the following composition of functions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    N(x) &= \\sigma(\\textbf{w}_{1}^C \\cdot \\textbf{x}_{1}^C + b_{1}^C) \\\\\n",
    "         &= TODO\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
