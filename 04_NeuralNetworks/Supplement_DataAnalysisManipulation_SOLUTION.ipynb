{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b7d20",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Analysis and Manipulation\n",
    "\n",
    "In the [previous notebook](./Introduction_NeuralNetworks.ipynb), we learned that given sufficient data, time, and network complexity, neural networks are\n",
    "capable of approximating continuous functions defined on compact sets to an arbitrary degree. Of course, any neural network we train can only ever be as good\n",
    "as the data we use to train it; that is, if we hope to train a neural network which generalizes some phenomenon beyond the dataset it was trained on,\n",
    "we must ensure that the data used to train the network accurately represents the phenomenon we wish to model. In order to determine whether a particular dataset\n",
    "is well-constructed, we must understand the data set. For this reason, we direct our focus in this notebook toward basic tooling to help transform,\n",
    "visualize, and analyze various datasets by way of the excellent open-source [pandas](https://pandas.pydata.org/) library. In particular, we will modify the\n",
    "well-known MNIST database of handwritten digits into a format which is compatible with PyTorch and create a neural network to automatically identify handwritten\n",
    "digits.\n",
    "\n",
    "### Software Prerequisites\n",
    "\n",
    "The following Python libraries are prerequisites to run this notebook; simply run the following code block to install them. They are also listed in the `requirements.txt` file in the root of this notebook's [GitHub repository](https://github.com/uccs-math-clinic/mc-workshops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79290e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 \\\n",
    "             numpy==1.21.5 \\\n",
    "             torch==1.11.0 \\\n",
    "             torchvision==0.12.0 \\\n",
    "             pandas==1.3.5 \\\n",
    "             Pillow==9.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20198ac",
   "metadata": {},
   "source": [
    "The Python kernel must be restarted after running the above code block for the first time within a particular virtual environment. This may be accomplished by navigating to `Kernel -> Restart` in the menu bar.\n",
    "\n",
    "With our package dependencies installed, we can run the following [boilerplate code](https://en.wikipedia.org/wiki/Boilerplate_code) in order to import the packages needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae5321",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8701da3",
   "metadata": {},
   "source": [
    "## The MNIST Handwritten Digit Dataset\n",
    "\n",
    "The [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) is a collection of digitized handwritten digits which was published by the National Institute\n",
    "of Standards and Technology in 1998 as a collection of images which was (at the time) ill-suited for machine learning due to the diversity and inconsistency of\n",
    "handwriting styles which comprise the dataset. For this reason, modeling the MNIST database of handwritten digits with a high degree of accuracy eluded researchers\n",
    "for quite some time. Modern machine learning techniques (particularly convolutional neural networks) have made this task quite simple, and hence this dataset\n",
    "serves as an excellent starting point for learning how to model a real-world problem with neural networks.\n",
    "\n",
    "We begin by examining the original dataset as presented to researchers. We'll first write some code to convert files in this dataset to a _comma-separated values_\n",
    "(CSV) file, and then we'll load that file into a Numpy array for use in a PyTorch model.\n",
    "\n",
    "The original dataset contains two sets of files: one for training data, and the other for model validation. Each of these sets contains a file with image _labels_\n",
    "and a file with the image _data_. Each label is represented by a digit between 0-9, and each image is represented as a list of 784 numbers (corresponding to\n",
    "a 28x28 pixel image) which take a value between 0-255. This value in turn represents the pixel's grayscale value, with 0 being the darkest and 255 being the\n",
    "lightest. It is important to note that these files do not contain _images_ - instead, they contain the image's _pixel data_. This means that we can't just use\n",
    "any old image viewer to see what these digits look like; we'll need to write some code to do to that for us. Along the way, we'll gain a little more understanding\n",
    "about how the image files with which we are familiar are encoded.\n",
    "\n",
    "The first file that we'll examine is [t10k-labels.idx1-ubyte](./t10k-labels.idx1-ubyte), which contains image labels for our test set. The first eight bytes of\n",
    "this file represents some metadata about the file as well as the number of labels that the file contains. Each byte thereafter indicates which number the\n",
    "corresponding test image represents. For example, the ninth byte (byte `09`) indicates which number the first image represents. The tenth byte contains the\n",
    "numerical label for the second image, and so on and so forth.\n",
    "\n",
    "Similarly, the second file [t10k-images.idx3-ubyte](./t10k-images.idx3-ubyte) contains the image test data itself. The first 16 bytes of this file contains file\n",
    "metadata and indicates the number of images and corresponding image sizes. The next 784 bytes (i.e., bytes `16-800`) represent the pixel values for the first\n",
    "test image (whose label is indicated by byte `09` from the previous label file) and the 784 bytes after that (i.e., bytes `801 - 1584`) represent the second test\n",
    "image. In total, there are 10,000 images and corresponding labels encoded into these two files.\n",
    "\n",
    "More information about these data sets can be found at the [original source](http://yann.lecun.com/exdb/mnist/). In the meantime, let's use our knowledge about\n",
    "these files to create more array- and matrix-friendly data representations of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd1864",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Open files for reading - this lets us read the contents of these files programmatically\n",
    "# as bytes.\n",
    "#\n",
    "test_labels_file = open('./t10k-labels.idx1-ubyte', 'rb')\n",
    "test_images_file = open('./t10k-images.idx3-ubyte', 'rb')\n",
    "\n",
    "# Open output CSV file for writing.\n",
    "#\n",
    "out_csv_file = open('./mnist_test.csv', 'w')  # This is the file we'll be generating.\n",
    "\n",
    "# First, let's read past the first 8 bytes of the label file to skip past the file metadata.\n",
    "#\n",
    "test_labels_file.read(8)\n",
    "\n",
    "# We'll do the same thing for the first 16 bytes of the test file metadata.\n",
    "#\n",
    "test_images_file.read(16)\n",
    "\n",
    "# This array will contain our image pixel values. Since we're reading 10,000 images, this\n",
    "# array will contain 10,000 entries.\n",
    "#\n",
    "images = []\n",
    "\n",
    "# For each of the 10,000 images, we need to read 784 bytes from the images file and one byte\n",
    "# from the labels file.\n",
    "#\n",
    "for i in range(10000):\n",
    "    # Read i^th image label\n",
    "    #\n",
    "    image_label = [ord(test_labels_file.read(1))]\n",
    "\n",
    "    # Read i^th image pixels\n",
    "    #\n",
    "    image_pixels = []\n",
    "    for j in range(784):\n",
    "        image_pixels.append(ord(test_images_file.read(1)))\n",
    "\n",
    "    # Add label and pixels to \"images\" array. The \"+\" operator here\n",
    "    # simply concatenates the \"image_label\" and \"image_pixels\" arrays\n",
    "    # together.\n",
    "    #\n",
    "    images.append(image_label + image_pixels)\n",
    "\n",
    "# Let's print the first array value just to see what we have at this point as a\n",
    "# sanity check.\n",
    "#\n",
    "print('First image label: {}'.format(images[0][0]))\n",
    "print('First image pixel values:')\n",
    "[print(images[0][k*28+1:k*28+28]) for k in range(round((len(images[0]) - 1)/28))]\n",
    "\n",
    "# We'll add column headers to the CSV file here.\n",
    "#\n",
    "out_csv_file.write('label,')\n",
    "out_csv_file.write(','.join('pixel_{}'.format(i) for i in range(784)) + '\\n')\n",
    "\n",
    "# Now, let's comma-separate each image array value.\n",
    "#\n",
    "for image_data in images:\n",
    "    out_csv_file.write(\n",
    "        ','.join(str(image_datum) for image_datum in image_data) + '\\n'\n",
    "    )\n",
    "\n",
    "# Close files\n",
    "#\n",
    "test_images_file.close()\n",
    "test_labels_file.close()\n",
    "\n",
    "out_csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a7704",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Interlude - Visualizing Our Data\n",
    "\n",
    "Now that we have our MNIST data as a nicely-formatted CSV, we can use some of the more popular Python libraries to dig into our data to begin to get\n",
    "a feel for its structure. The library we have in mind is [pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html),\n",
    "which provides a wide variety of useful data exploration capabilities. We'll begin by loading our generated CSV file into a\n",
    "[pandas.DataFrame](https://pandas.pydata.org/docs/reference/frame.html) instance. The `pandas` library provides a convenient `read_csv` function just for\n",
    "this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767a0d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist_df = pd.read_csv('./mnist_test.csv')\n",
    "\n",
    "mnist_df.head()  # This function prints the first 10 rows of our data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d11abc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Restricting Columns\n",
    "\n",
    "We might not always want to see every column in a particular CSV file. Fortunately, `pandas` provides an easy way to only view a particular column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc77af3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist_df['label']  # Only print the column labeled \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6001b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Label Counts\n",
    "\n",
    "We can then calculate counts of each digit number using the `value_counts` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fd682",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216df6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Normalized Label Distribution\n",
    "\n",
    "This gives us the raw numbers sorted by decreasing frequency. Let's look as the relative frequency of each digit by normalizing the data\n",
    "and sorting by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc0239",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist_df['label'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8ce52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338e839",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist_df.drop('label', axis=1).stack().value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fb095",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These few capabilities comprise only a very small subset of the full set of analytics capabilities that Data Frames provide. The full documentation for this\n",
    "powerful tool can be found [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), and the interested reader is highly encouraged to\n",
    "experiment with some other DataFrame methods on our MNIST data set. For now, however, we'll complete our brief survey of data analytics by actually plotting\n",
    "an image!\n",
    "\n",
    "### Plotting an Image\n",
    "\n",
    "Recall that our data in its current form is structured as a list of rows which contain the image label which is proceeded by 784 values which indicate the\n",
    "grayscale shade of a particular pixel. Thus, in order to visualize a particular image in a more familiar way, we need to convert our image from a row of pixel\n",
    "values into a 28x28 pixel grid. Fortunately, `pandas` utilizes the `numpy` library under the hood, which means that we can take advantage of that fact to easily\n",
    "reshape our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df356c7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# iloc[0] selects the first image (index 0) from our DataFrame. Feel free to\n",
    "# replace \"0\" with any other number between 0-9999 to view a different image.\n",
    "#\n",
    "image_row_vector = mnist_df.drop('label', axis=1).iloc[0].values\n",
    "\n",
    "# We use numpy's .reshape method to turn our row vector into a 28x28 matrix.\n",
    "#\n",
    "image_matrix = image_row_vector.reshape(28, 28)\n",
    "\n",
    "# Now we plot our pixel matrix as a grayscale image\n",
    "#\n",
    "fig = plt.matshow(image_matrix, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8043d9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Loading Data into PyTorch\n",
    "\n",
    "At this point, we are ready to load data into PyTorch in order to train a neural network to classify all these digits. To do so, we need to convert our array\n",
    "of images to a PyTorch `Dataset` instance. We'll create our own `MNISTTestDataset` class which extends the `Dataset` class (provided by PyTorch). This allows\n",
    "PyTorch to interface with our data in a standard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690a76c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTTestDataset(Dataset):\n",
    "    def __init__(self, mnist_dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mnist_dataframe (pd.DataFrame): pandas DataFrame containing test data\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.mnist_dataframe = mnist_dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_label = self.mnist_dataframe.iloc[index, 0]\n",
    "\n",
    "        # Images are almost always loaded into PyTorch via the PIL library,\n",
    "        # so we're going to create a PIL image here from the raw pixel values\n",
    "        # we read so that our data types and format are consistent with standard\n",
    "        # practices.\n",
    "        #\n",
    "        img = Image.fromarray(\n",
    "            self.mnist_dataframe.iloc[index, 1:]\n",
    "                .to_numpy()\n",
    "                .reshape(28, 28)\n",
    "                .astype('uint8'),\n",
    "            mode=\"L\",\n",
    "        )\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, item_label\n",
    "\n",
    "# Create a transformer for our images. This one converts\n",
    "# an image's pixel values into a 1x28x28 tensor, which\n",
    "# represents a 28x28 pixel image.\n",
    "#\n",
    "image_transformer = transforms.ToTensor()\n",
    "\n",
    "# This is where we create our actual test dataset instance. PyTorch actually provides\n",
    "# a very simple interface for downloading and converting MNIST data to PyTorch Datasets\n",
    "# out of the box, so we're going to use that for our training data. We could\n",
    "# have easily done the same for our test data, but we wouldn't have learned nearly as\n",
    "# much along the way!\n",
    "#\n",
    "custom_test_data = MNISTTestDataset(mnist_df, transform=image_transformer)\n",
    "dl_train_data = datasets.MNIST('./data', train=True, download=True, transform=image_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81de87",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating the Neural Network\n",
    "\n",
    "Now that we've assembled our data, we're ready to use PyTorch to train a neural network to identify these digits. The first thing we'll need is a PyTorch\n",
    "`DataLoader`, which is what PyTorch uses to load a subset of the total dataset for training. From there, we'll define our neural network as a PyTorch `Module`,\n",
    "which allows PyTorch to automatically instantiate a network of connected activation functions - just like we did \"by hand\" in the previous workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674474bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_loader_batch_size = 100  # Train on 100 images at a time\n",
    "\n",
    "mnist_train_data_loader = DataLoader(\n",
    "    dataset=dl_train_data,\n",
    "    batch_size=data_loader_batch_size,\n",
    "    shuffle=True  ## We'll shuffle images around from their initial downloaded order.\n",
    ")\n",
    "\n",
    "mnist_test_data_loader = DataLoader(\n",
    "    dataset=custom_test_data,\n",
    "    batch_size=data_loader_batch_size,\n",
    "    shuffle=True  ## No need to shuffle these, since we're only using these to test model accuracy.\n",
    ")\n",
    "\n",
    "# Next, we'll create a neural network with a single hidden layer.\n",
    "# We do so by extending PyTorch's Module class. We'll add some\n",
    "# constructor arguments to this class in order to parameterize the\n",
    "# neural network size.\n",
    "#\n",
    "class MNISTNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_layer_size, hidden_layer_size, output_layer_size, activation_function):\n",
    "        super().__init__()\n",
    "\n",
    "        # We define our input layer as a linear function - this calculates\n",
    "        # our w*x + b values for the first layer\n",
    "        #\n",
    "        self.input_layer = nn.Linear(input_layer_size, hidden_layer_size)\n",
    "\n",
    "        # This is the activation function for our hidden layer.\n",
    "        #\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Similar to the input layer, this layer calculates w*x + b values for the\n",
    "        # final layer.\n",
    "        #\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, output_layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how forward propagation should be executed in this network.\n",
    "\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = self.input_layer(x)\n",
    "        out = self.activation_function(out)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        # The above is exactly the same as composing our layers as functions:\n",
    "        #\n",
    "        # return self.output_layer(self.activation_function(self.input_layer(x)))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b79e40",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "We're finally ready to train the neural network to see how it performs! We'll define the training algorithm as a function so that we can easily change\n",
    "various training parameters (such as the learning rate, optimization algorithm, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ce18e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_nn(train_data_loader, loss_fn, activation_function, epochs,\n",
    "             learning_rate=0.01,\n",
    "             hidden_layer_size=500,\n",
    "             device='cpu'):\n",
    "    # Instantiate our neural network\n",
    "    #\n",
    "    model = MNISTNeuralNetwork(\n",
    "        input_layer_size=784,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        output_layer_size=10,\n",
    "        activation_function=activation_function,\n",
    "    ).to(device)\n",
    "\n",
    "    # We'll use our favorite Stochastic Gradient Descent optimizer\n",
    "    # in this neural network.\n",
    "    #\n",
    "    optimizer=optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # We can alternatively use the ADAM optimizer, which shows somewhat\n",
    "    # faster convergence than SGD at the cost of model generalizability.\n",
    "    #\n",
    "    # More information can be found in the original paper: https://arxiv.org/abs/1412.6980\n",
    "    #\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train our neural network\n",
    "    #\n",
    "    total_train_steps = len(train_data_loader)\n",
    "    for epoch in range(epochs):\n",
    "        for k, (image_batch, label_batch) in enumerate(train_data_loader):\n",
    "            # Reshape 28x28 pixel image to a single 784-element array\n",
    "            #\n",
    "            device_images = image_batch.reshape(-1, 784).to(device)\n",
    "\n",
    "            # No need to reshape labels, since our labels are just single\n",
    "            # digits.\n",
    "            #\n",
    "            actual_labels = label_batch.to(device)\n",
    "\n",
    "            # Forward propagation\n",
    "            #\n",
    "            predictions = model(device_images)\n",
    "            loss = loss_fn(predictions, actual_labels)\n",
    "\n",
    "            # Backpropagation\n",
    "            #\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print some progress information along with current model loss\n",
    "            # every 100 steps.\n",
    "            #\n",
    "            if (k + 1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch + 1,\n",
    "                    epochs,\n",
    "                    k + 1,\n",
    "                    total_train_steps,\n",
    "                    loss.item(),\n",
    "                ))\n",
    "\n",
    "    # Use our model to predict test images from the dataset. This gives us an\n",
    "    # idea about how accurate our model is; we want this number to be as high\n",
    "    # as possible!\n",
    "    #\n",
    "    #\n",
    "    with torch.no_grad():   # This prevents PyTorch from calculating gradients during prediction.\n",
    "        total_test_images = 0\n",
    "        correctly_classified_images = 0\n",
    "\n",
    "        for image_batch, label_batch in mnist_test_data_loader:\n",
    "            # Send this image batch to the destination device\n",
    "            #\n",
    "            device_images = image_batch.reshape(-1, 784).to(device)\n",
    "            actual_labels = label_batch.to(device)\n",
    "\n",
    "            # Use trained neural network model to classify test images\n",
    "            #\n",
    "            predictions = model(device_images)\n",
    "            _, predicted_labels = torch.max(predictions.data, 1)\n",
    "\n",
    "            # Calculate number of correct digit classifications\n",
    "            #\n",
    "            total_test_images += actual_labels.size(0)\n",
    "            correctly_classified_images += (predicted_labels == actual_labels).sum().item()\n",
    "\n",
    "        print('Final model accuracy over all test images is {}%'.format(\n",
    "            100.0 * correctly_classified_images / total_test_images\n",
    "        ))\n",
    "\n",
    "\n",
    "# Let's try our neural network out with the usual sigmoid activation function.\n",
    "# We're using the CrossEntropyLoss function to calculate our model loss, as it\n",
    "# tends to be a more useful loss function for classification problems. More\n",
    "# information can be found at the PyTorch documentation:\n",
    "#\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
    "#\n",
    "train_nn(\n",
    "    train_data_loader=mnist_train_data_loader,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    activation_function=nn.Sigmoid(),\n",
    "    epochs=2,\n",
    "    learning_rate=0.1,\n",
    "    hidden_layer_size=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d7aba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make it work - _then_ make it work better\n",
    "\n",
    "Now that we have some exposure to the kinds of tooling that we can use for data analysis and machine learning,\n",
    "let's see how well we can get this neural network to perform! The model we introduced managed ~90% test accuracy\n",
    "on this training set, but much higher numbers are quite achievable -\n",
    "[one 2020 model even achieved a 99.91% accuracy(!)](https://arxiv.org/abs/2008.10400v2) on this dataset. This\n",
    "notebook's exercise is to try out various activation functions (e.g., hyperbolic tangent functions), optimizers,\n",
    "learning rates, and neural network architectures (e.g., adding more hidden layers) to see how much better we\n",
    "can score on the test dataset. There's no correct answer or solution - this notebook is all about exploring ideas\n",
    "in order to see what does and doesn't work for this data set.\n",
    "\n",
    "Happy learning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
