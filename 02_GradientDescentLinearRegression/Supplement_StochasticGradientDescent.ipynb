{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In this suplementary section, we introduce the powerful idea of stochastic gradient descent. At each iteration  (learning step) we compute the gradient direction based on a subset of the data, which we call the training data, rather than on the entire data set. Thus, the decrease in the loss fsunction is not guaranteed at each iteration. This may seem like a disadvantage at first, since there may be more iterations needed to achieve the same level of accuracy, but there is interesting theory to back up such a strategy which benefits from decrease in the compute required, and also it assures avoiding the possibility of overfitting the data.\n",
    "\n",
    "### Software Prerequisites\n",
    "\n",
    "The following Python libraries are prerequisites to run this notebook; simply run the following code block to install them. They are also listed in the `requirements.txt` file in the root of this notebook's [GitHub repository](https://github.com/uccs-math-clinic/mc-workshops).\n",
    "\n",
    "When you run this code block for the first time, you will need to restart the kernel by navigating to `Kernel -> Restart` in the menu bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 \\\n",
    "             numpy==1.21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our package dependencies installed, we can run the following cell in order to import the packages needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import a few different libraries that make our work a bit easier.\n",
    "# We also give them each aliases (the part after \"as\") which make them\n",
    "# a little easier to remember; the ones shown below are those used \n",
    "# most commonly, but you can call these whatever you want! This practice\n",
    "# is quite prevalent in Python as a whole, and doubly so in data science.\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# In addition to our usual imports, we add a tool to split our dataset into\n",
    "# training data (used to compute the gradient direction) and testing data \n",
    "# (which can be used for validation, although it will not happen here).\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This allows us to run animations in this notebook; this isn't necessary\n",
    "# for the vast majority of notebooks, but it does serve as a useful teaching\n",
    "# tool.\n",
    "#\n",
    "%matplotlib notebook\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Implementation\n",
    "\n",
    "The following code block implements stochastic gradient descent by calculating the gradient from a subset of the data at each iteration; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next few lines of code simply are reproduced from the gradient descent cell above, as initiating the steps\n",
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Since our gradient calculation is a bit non-trivial, we're\n",
    "# going to put that calculation into its own function. This\n",
    "# code is nothing more than the gradient function defined\n",
    "# above.\n",
    "#\n",
    "def calculate_gradient(slope, intercept, x_vals, y_vals):\n",
    "    # Calculate mean standard error gradient\n",
    "    #\n",
    "    abs_error = (slope * x_vals + intercept) - y_vals\n",
    "    d_slope = np.sum(2 * abs_error * x_vals) / len(x_vals)\n",
    "    d_intercept = np.sum(2 * (abs_error)) / len(x_vals)\n",
    "    \n",
    "    # Calculate mean standard error value\n",
    "    #\n",
    "    mse = np.sum(np.power(abs_error, 2)) / len(x_vals)\n",
    "    \n",
    "    return (d_slope, d_intercept, mse)\n",
    "\n",
    "left_xlim = -5\n",
    "right_xlim = 5\n",
    "\n",
    "theta_1 = 0\n",
    "theta_2 = 0\n",
    "\n",
    "convergence_error_threshold = 0.1\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "[m, b] = np.random.randint(1, 10, size=2)\n",
    "x = np.linspace(left_xlim, right_xlim, 100)[:, np.newaxis]\n",
    "y = m * x + b + (2 * np.random.randn(100, 1))\n",
    "ax.scatter(x, y, color='black')\n",
    "\n",
    "dtheta_1, dtheta_2, err = calculate_gradient(theta_1, theta_2, x, y)\n",
    "\n",
    "y_train_predicted = theta_1 * x + theta_2\n",
    "z, = ax.plot(x, y_train_predicted, color='orange')\n",
    "\n",
    "ax.set_ylim(bottom=-5)\n",
    "\n",
    "loss_old = err\n",
    "loss_val=loss_old\n",
    "\n",
    "# Now is the iterative step, where at each iteration we split the data.\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.15)\n",
    "\n",
    "    ax.scatter(x_train, y_train, color='black')\n",
    "    ax.scatter(x_test, y_test, color='red')\n",
    "    \n",
    "    # Calculate the new gradient and corresponding line based on the training data only\n",
    "    \n",
    "    dtheta_1, dtheta_2, err = calculate_gradient(theta_1, theta_2, x_train, y_train)\n",
    "          \n",
    "    theta_1 = theta_1 - (learning_rate * dtheta_1)\n",
    "    theta_2 = theta_2 - (learning_rate * dtheta_2)\n",
    "    \n",
    "    y_train_predicted = theta_1 * x_train + theta_2\n",
    "    \n",
    "    # Plot new line values - as before, these lines are mostly for\n",
    "    # animation purposes.\n",
    "    #\n",
    "    z.set_xdata(x_train)\n",
    "    z.set_ydata(y_train_predicted)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    \n",
    "    # Comment out this line if you want to see how fast this can converge!\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "    # For the termination condition, we need to compute the value of the loss function\n",
    "    # and also set the acceptable min loss change\n",
    "    \n",
    "    loss_new = err\n",
    "\n",
    "    acceptable_min_delta_loss = 0.005*loss_new\n",
    "    \n",
    "    if abs(loss_new - loss_old) < acceptable_min_delta_loss:\n",
    "        print('We converged to our specified tolerance in {} iterations!'.format(i))\n",
    "        break\n",
    "           \n",
    "    loss_old = loss_new       \n",
    "\n",
    "    loss_val= np.append(loss_val, loss_old)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(loss_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
