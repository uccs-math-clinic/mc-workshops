{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In this suplementary section, we introduce the powerful idea of stochastic gradient descent. At each iteration  (learning step) we compute the gradient direction based on a subset of the data, which we call the training data, rather than on the entire data set. Thus, the decrease in the loss fsunction is not guaranteed at each iteration. This may seem like a disadvantage at first, since there may be more iterations needed to achieve the same level of accuracy, but there is interesting theory to back up such a strategy which benefits from decrease in the compute required, and also it assures avoiding the possibility of overfitting the data.\n",
    "\n",
    "### Software Prerequisites\n",
    "\n",
    "The following Python libraries are prerequisites to run this notebook; simply run the following code block to install them. They are also listed in the `requirements.txt` file in the root of this notebook's [GitHub repository](https://github.com/uccs-math-clinic/mc-workshops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 \\\n",
    "             numpy==1.21.5 \\\n",
    "             scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python kernel must be restarted after running the above code block for the first time within a particular virtual environment. This may be accomplished by navigating to `Kernel -> Restart` in the menu bar.\n",
    "\n",
    "With our package dependencies installed, we can run the following cell in order to import the packages needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import a few different libraries that make our work a bit easier.\n",
    "# We also give them each aliases (the part after \"as\") which make them\n",
    "# a little easier to remember; the ones shown below are those used \n",
    "# most commonly, but you can call these whatever you want! This practice\n",
    "# is quite prevalent in Python as a whole, and doubly so in data science.\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# In addition to our usual imports, we add a tool to split our dataset into\n",
    "# training data (used to compute the gradient direction) and testing data \n",
    "# (which can be used for validation, although it will not happen here).\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This allows us to run animations in this notebook; this isn't necessary\n",
    "# for the vast majority of notebooks, but it does serve as a useful teaching\n",
    "# tool.\n",
    "#\n",
    "%matplotlib notebook\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Implementation\n",
    "\n",
    "The following code block implements stochastic gradient descent by calculating the gradient from a subset of the data at each iteration; the data used to calculate the gradient is colored with red dots in the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our standard MSE gradient calculation.\n",
    "#\n",
    "def calculate_gradient(slope, intercept, x_vals, y_vals):\n",
    "    # Calculate mean standard error gradient\n",
    "    #\n",
    "    abs_error = (slope * x_vals + intercept) - y_vals\n",
    "    d_slope = np.sum(2 * abs_error * x_vals) / len(x_vals)\n",
    "    d_intercept = np.sum(2 * (abs_error)) / len(x_vals)\n",
    "\n",
    "    # Calculate mean standard error value\n",
    "    #\n",
    "    mse = np.sum(np.power(abs_error, 2)) / len(x_vals)\n",
    "\n",
    "    return (d_slope, d_intercept, mse)\n",
    "\n",
    "def sgd(m, b, left_xlim=-3, right_xlim=3, train_iterations=50, test_size=0.15):\n",
    "\n",
    "    fig, [ax, err_ax] = plt.subplots(2, 1)\n",
    "    ax.set_title('Predicted Line')\n",
    "    err_ax.set_title('Error (MSE)')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "    # Initial parameter (slope and y-intercept) guesses\n",
    "    #\n",
    "    theta1 = 0\n",
    "    theta2 = 0\n",
    "\n",
    "    convergence_error_threshold = 0.1\n",
    "\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    x = np.linspace(left_xlim, right_xlim, 100)[:, np.newaxis]\n",
    "    y = m * x + b + (2 * np.random.randn(100, 1))\n",
    "    ax.scatter(x, y, color='black')\n",
    "\n",
    "    dtheta1, dtheta2, err = calculate_gradient(theta1, theta2, x, y)\n",
    "    \n",
    "    err_vals = [err]\n",
    "\n",
    "    y_train_predicted = theta_1 * x + theta_2\n",
    "    z, = ax.plot(x, y_train_predicted, color='orange')\n",
    "    e, = err_ax.plot(np.arange(0, len(err_vals)), err_vals)\n",
    "    \n",
    "    err_ax.set_ylim(bottom=0)\n",
    "    err_ax.set_xlim(left=0, right=train_iterations)\n",
    "    \n",
    "    previous_loss = err\n",
    "\n",
    "    # Now is the iterative step, where at each iteration we split the data and train\n",
    "    # only on the split data.\n",
    "    #\n",
    "    for i in range(train_iterations):\n",
    "        # Split training data\n",
    "        #\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
    "\n",
    "        ax.scatter(x_train, y_train, color='black')\n",
    "        ax.scatter(x_test, y_test, color='red')\n",
    "\n",
    "        # Calculate the new gradient and corresponding line based on the training data only\n",
    "        # (i.e., _not_ the entire data set)\n",
    "        #\n",
    "        dtheta1, dtheta2, err = calculate_gradient(theta1, theta2, x_train, y_train)\n",
    "\n",
    "        theta1 = theta1 - (learning_rate * dtheta1)\n",
    "        theta2 = theta2 - (learning_rate * dtheta2)\n",
    "\n",
    "        y_train_predicted = theta1 * x_train + theta2\n",
    "        err_vals.append(err)\n",
    "\n",
    "        # Plot new line values - as before, these lines are mostly for\n",
    "        # animation purposes.\n",
    "        #\n",
    "        z.set_xdata(x_train)\n",
    "        z.set_ydata(y_train_predicted)\n",
    "        e.set_xdata(np.arange(0, len(err_vals)))\n",
    "        e.set_ydata(err_vals)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "        # Comment out this line if you want to see how fast this can converge!\n",
    "        time.sleep(0.05)\n",
    "\n",
    "        # We may also choose to terminate if the loss changes by less than 5%.\n",
    "        # Just set terminate_early to True to see how this affects training.\n",
    "        #\n",
    "        terminate_early = False\n",
    "        if abs(err - previous_loss) < 0.005 * err and terminate_early:\n",
    "            print('We converged to our specified tolerance in {} iterations!'.format(i))\n",
    "            break\n",
    "        \n",
    "    print('Learned slope after {} iterations is {}; control value was {}.'.format(\n",
    "        train_iterations,\n",
    "        theta1,\n",
    "        m,\n",
    "    ))\n",
    "\n",
    "    print('Learned y-intercept after {} iterations is {}; control value was {}.'.format(\n",
    "        train_iterations,\n",
    "        theta2,\n",
    "        b,\n",
    "    ))\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "sgd_fig, sgd_ax = sgd(2, 1, train_iterations=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
