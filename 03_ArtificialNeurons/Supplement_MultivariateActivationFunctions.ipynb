{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b7d20",
   "metadata": {},
   "source": [
    "## Multi-Dimensional Decision Inputs\n",
    "\n",
    "In the [Introduction to Activation Functions](Introduction_ActivationFunctions.ipynb) notebook, we\n",
    "introduced the idea that very simple decisions may be encoded into mathematical functions known as\n",
    "_activation functions_. These functions in general take some numerical value as input and output\n",
    "a number between 0 and 1, which we interpret to be a measure of confidence about whether the decision\n",
    "which the function had made is a \"yes\" decision or a \"no\" decision. Our examples for this were quite\n",
    "limited in nature - we necessarily chose to consider examples which embodied a very simple in/out\n",
    "relationship consisting of a single variable input and a single output. It is immediately apparent that\n",
    "such models are extremely limited in their scope of applicability to real-world problems, as very few\n",
    "phenomena in the world around us are simple enough to be modeled via a simple activation function. In\n",
    "general, the vast majority of observed effects have multiple contributing causes, each of which contributes\n",
    "to the observed effect to varying degrees. For instance, one does not consider only the ambient\n",
    "temperature outside when deciding what to wear into a rainstorm - surely precipitation should\n",
    "influence this decision too, and to a disproportionate degree at that! It is intuitively apparent then\n",
    "that any model which we wish to use in order to replicate, classify, or predict observed phenomena ought to\n",
    "account for multiple causes.\n",
    "\n",
    "In this notebook, we expand our rudimentary decision model in a simple yet profound way by allowing multiple\n",
    "inputs to help determine what decision a model makes. In doing so, we take a small but significant step\n",
    "toward assembling a mathematical model which is capable of learning how to accurately make mathematically\n",
    "nuanced decisions.\n",
    "\n",
    "\n",
    "### Software Prerequisites\n",
    "\n",
    "The following Python libraries are prerequisites to run this notebook; simply run the following code block to\n",
    "install them. They are also listed in the `requirements.txt` file in the root of this notebook's\n",
    "[GitHub repository](https://github.com/uccs-math-clinic/mc-workshops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edab814",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 \\\n",
    "             numpy==1.21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e807e1",
   "metadata": {},
   "source": [
    "The Python kernel must be restarted after running the above code block for the first time within a particular virtual environment. This may be accomplished by navigating to `Kernel -> Restart` in the menu bar.\n",
    "\n",
    "With our package dependencies installed, we can run the following [boilerplate code](https://en.wikipedia.org/wiki/Boilerplate_code) in order to import the packages needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa302706",
   "metadata": {},
   "source": [
    "## Input Features\n",
    "\n",
    "As we explore a particular phenomenon which we wish to model or predict, it is likely that we can make some pretty\n",
    "good guesses as to what sorts of things contribute to the behavior of the phenomenon we observe. For example, if we\n",
    "consider a model which predicts appropriate outerwear given a set of weather conditions, we might conjecture that\n",
    "temperature, humidity, precipitation, and wind speed are all relevant sources of information for making an informed\n",
    "choice. If instead we wish to predict a student's performance on a test, we might consider the number of hours studied\n",
    "beforehand, the length of time over which the test material was presented in class, and the student's sleep schedule to\n",
    "all be contributing factors toward a pass or fail result. In both of these scenarios, there exist several contributing\n",
    "factors toward a particular result, and each factor carries with it its own level of influence toward that final result.\n",
    "These distinct factors are referred to as _features_ (or variables or attributes) for a mathematical model and - as we've\n",
    "seen with our simple examples already - may vary quite a bit between various models and contexts. The degree to which a\n",
    "particular feature contributes to a final decision made is referred to as that feature's _weight_. We often also wish to\n",
    "maintain some control over the threshold at which a decision is made; for instance, we'd probably like for a model which\n",
    "predicts the likelihood of a safe lane change while driving to output a very high probably of success before we act on\n",
    "that prediction. This activation threshold is referred to as the feature's _bias_. By adjusting these weights and biases,\n",
    "our model encodes the degree to which distinct input features are prioritized and in doing so allows us to more closely align\n",
    "the model's behavior with our own intuition and observations.\n",
    "\n",
    "Recall that we chose the logistic function as our sigmoid activation function, whose definition and derivative are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "           f(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "  \\frac{df}{dz} &= f(z) \\cdot (1 - f(z))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Moreover, if $z = z(\\boldsymbol{\\theta}, x)$ is also function of some set of parameters $\\boldsymbol{\\theta} = \\{\\theta_{i}\\}_{i=1}^{n}$ in\n",
    "addition to the input value $x$, then the partial derivative with respect to any single parameter $\\theta_i$ may be calculated\n",
    "via the Chain Rule:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial f}{\\partial \\theta_i} &= f(z) \\cdot (1 - f(z)) \\cdot \\frac{\\partial z}{\\partial \\theta_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In our formulation of the sigmoid function as a decision function, we let $z = wx + b$ and chose $w$ and $b$ to represent _weight_ and _bias_\n",
    "**parameters** for the activation function $\\sigma(z)$ (in this case, we chose $\\theta_1 = w$ and $\\theta_2 = b$), and $x$ the actual feature data point.\n",
    "We now extend this formulation to a decision function which maps two feature inputs $x_1$ and $x_2$ (with corresponding\n",
    "weights $w_1$ and $w_2$ and biases $b_1$ and $b_2$) to a single output decision. As before, we wish to learn the weight and bias which\n",
    "allows our model to most closely represent the available data set; the primary difference at this point is, of course, the fact that we are\n",
    "learning two sets of weights and biases simultaneously instead of just one. In this case, our decision function looks quite similar to before,\n",
    "with a slight modification to our definition of $z$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "           f(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "                &= \\frac{1}{1 + e^{-(w_{1}x_{1} + b_1 + w_{2}x_{2} + b_2)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In general, for $n$ input features, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "                                           f(z) &= \\frac{1}{1 + e^{-\\sum_{i=1}^{n}{w_{i}x_{i} + b_i}}} \\\\\n",
    "           \\frac{\\partial f}{\\partial w_i} &= f(z) \\cdot (1 - f(z)) \\cdot x_i \\\\\n",
    "           \\frac{\\partial f}{\\partial b_i} &= f(z) \\cdot (1 - f(z))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We may further simplify calculations with the observation that $\\frac{\\partial f}{\\partial b_i} = \\frac{\\partial f}{\\partial b_j}$ for every\n",
    "$1 \\le i, j  \\le n$. We thus only need to calculate this derivative once; for this reason, we may combine (sum) every $b_i$ in the above\n",
    "equation and treat the result as a single bias parameter $b = \\sum\\limits_{i=1}^{n}{b_i}$ which corresponds to the _entire_ input layer. In\n",
    "doing so, our new system simplifies to the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "                                           f(z) &= \\frac{1}{1 + e^{-b - \\sum_{i=1}^{n}{w_{i}x_{i}}}} \\\\\n",
    "           \\frac{\\partial f}{\\partial w_i} &= f(z) \\cdot (1 - f(z)) \\cdot x_i \\\\\n",
    "           \\frac{\\partial f}{\\partial b} &= f(z) \\cdot (1 - f(z))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## Learning a Multi-Feature Model\n",
    "\n",
    "In preparation for handling any finite number of input features, we utilize matrix computations for gradients in the below example;\n",
    "in doing so, we structure our training algorithm so that it is well-posed to take advantage of modern efficient matrix computations\n",
    "(which we will use extensively as our model grows in complexity and capability). Utilizing matrix computations of gradients when\n",
    "training these models has contributed in no small part to the (relatively) recent surge in popularity of machine learning. Prior to\n",
    "this innovation, neural network training was inefficient, difficult to scale, and impractical for most modern problems.\n",
    "\n",
    "We first revisit our sigmoid classification function and add to it the ability to classify based on multiple vectorized parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a711fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_classify(data_vectors, weight_vector, bias):\n",
    "    # We code our classification function manually for pedagogical reasons,\n",
    "    # but point out two equally valid and more elegant ways of performing the\n",
    "    # same task.\n",
    "\n",
    "    # We create a new list to hold our classification results, which for a particular\n",
    "    # element of x contains 1 for \"yes\" or 0 for \"no\".\n",
    "    #\n",
    "    classification_values = []\n",
    "\n",
    "    # Iterate through each value in x and add the corresponding classification result\n",
    "    # to the list of classification values.\n",
    "    #\n",
    "    for x_i in data_vectors:\n",
    "        # Calculate sigmoid function value from provided weight and bias terms\n",
    "        #\n",
    "        decision = 1 / (1 + np.exp(-1 * (np.sum(weight_vector * x_i) + np.sum(bias))))\n",
    "\n",
    "        classification_values.append(decision)\n",
    "\n",
    "    return np.array(classification_values)\n",
    "\n",
    "sample_data = np.array([\n",
    "    [7, 13],\n",
    "    [12, 11],\n",
    "    [7, 27],\n",
    "    [2, 3],\n",
    "])\n",
    "\n",
    "sample_weights = np.array([1, -1])\n",
    "sample_bias = np.array([2.0])\n",
    "\n",
    "decisions = sigmoid_classify(sample_data, sample_weights, sample_bias)\n",
    "for i in range(0, len(decisions)):\n",
    "    print('f(x={}, w={}, b={}): {}'.format(\n",
    "        sample_data[i],\n",
    "        sample_weights,\n",
    "        sample_bias,\n",
    "        decisions[i],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ebb4c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training the Sigmoid\n",
    "\n",
    "Now that our sigmoid function is capable of handling vectorized inputs, we are ready to implement gradient descent with our usual Mean Squared Error function.\n",
    "Before that, however, we digress briefly with a visualization of the state space; the reader is encouraged to modify the values in the `control_weights` and\n",
    " `control_biases` arrays to get a feel for how those values generate various sigmoid shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85bb9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_2d_sigmoid(weights, bias, left_xlim=-5, right_xlim=5):\n",
    "    mesh_distance = 20\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.set_title('Sigmoid Control Data')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    x = np.linspace(left_xlim, right_xlim, mesh_distance)\n",
    "    y = np.linspace(left_xlim, right_xlim, mesh_distance)\n",
    "\n",
    "    # We reshape x and y here to be compatible with the matrix format expected by\n",
    "    # the sigmoid_classify function as we defined it.\n",
    "    #\n",
    "    xy = np.array(np.meshgrid(x, y)).T.reshape(-1, 2)\n",
    "    z = sigmoid_classify(xy, np.array(weights), np.array(bias)) + np.random.uniform(-0.05, 0.05, xy[:,0].shape)\n",
    "\n",
    "    ax.scatter3D(xy[:,0], xy[:,1], z, c=z, cmap='Greens')\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "sig_fig, sig_ax = visualize_2d_sigmoid([ 0.5, 0.9 ], 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60015bf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As is custom, we leave the gradient value calculation to the reader to implement below.\n",
    "\n",
    "**Important!** Remember that we are encoding the gradient of the _loss function_, and not\n",
    "just the gradient of the sigmoid function! This means that we will need to sum over every\n",
    "point in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e28c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(weights, bias, xy_pairs, z_vals):\n",
    "    # Calculate the sigmoid activation function values. These are your predicted values given a\n",
    "    # particular list of (x, y) coordinate pairs (given in the \"xy\" function variable).\n",
    "    #\n",
    "    sigmoid_vals = sigmoid_classify(xy_pairs, weights, bias)\n",
    "\n",
    "    # Calculate the difference between the predicted values and the actual data.\n",
    "    #\n",
    "    abs_error = np.subtract(sigmoid_vals, z_vals)\n",
    "\n",
    "    # This is where you should encode the gradient of your loss function.\n",
    "    #\n",
    "\n",
    "    # d_weights should encode the partial derivatives of the _loss_ function with respect to each weight parameter.\n",
    "    #\n",
    "    d_weights = np.array([\n",
    "        0,\n",
    "        0,\n",
    "    ])\n",
    "\n",
    "    # d_bias should encode the partial derivative of the _loss_ function with respect to each bias parameter.\n",
    "    #\n",
    "    d_bias = 0\n",
    "\n",
    "    mse = np.sum(abs_error ** 2) / len(abs_error)\n",
    "\n",
    "    return d_weights, d_bias, mse\n",
    "\n",
    "# Everything below here is automatically done - no need to change\n",
    "# anything below this line, though you're encouraged to read through\n",
    "# it to better understand how to implement this algorithm!\n",
    "#\n",
    "def train_2d_sigmoid(control_weights, control_bias, left_xlim=-5, right_xlim=5, train_iterations=100):\n",
    "    fig, (err_ax) = plt.subplots(1, 1)\n",
    "\n",
    "    err_ax.set_title('Error (MSE)')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    mesh_distance = 20\n",
    "\n",
    "    x = np.linspace(left_xlim, right_xlim, mesh_distance)\n",
    "    y = np.linspace(left_xlim, right_xlim, mesh_distance)\n",
    "    xy = np.array(np.meshgrid(x, y)).T.reshape(-1, 2)\n",
    "    z = sigmoid_classify(\n",
    "            xy,\n",
    "            np.array(control_weights),\n",
    "            np.array(control_bias),\n",
    "        ) + np.random.uniform(-0.05, 0.05, xy[:,0].shape)\n",
    "\n",
    "    # Initial guess at weights and biases for our optimal sigmoid function\n",
    "    #\n",
    "    w = np.array([1.0, 1.0])\n",
    "    b = np.array([ 0.0 ])\n",
    "\n",
    "    learning_rate = 5.0\n",
    "\n",
    "    dw, db, err = calculate_gradient(w, b, xy, z)\n",
    "\n",
    "    err_vals = [err]\n",
    "    e, = err_ax.plot(np.arange(0, len(err_vals)), err_vals)\n",
    "\n",
    "    err_ax.set_ylim(bottom=0)\n",
    "    err_ax.set_xlim(left=0, right=train_iterations)\n",
    "\n",
    "    for i in range(train_iterations):\n",
    "        w = np.subtract(w, np.multiply(learning_rate, dw))\n",
    "        b = np.subtract(b, np.multiply(learning_rate, db))\n",
    "\n",
    "        dw, db, err = calculate_gradient(w, b, xy, z)\n",
    "        err_vals.append(err)\n",
    "\n",
    "        # Plot error\n",
    "        #\n",
    "        e.set_xdata(np.arange(0, len(err_vals)))\n",
    "        e.set_ydata(err_vals)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "    print('Learned weights after {} iterations are {} (control weights were {})'.format(\n",
    "        train_iterations,\n",
    "        w,\n",
    "        control_weights,\n",
    "    ))\n",
    "    print('Learned bias after {} iterations is {} (control bias was {})'.format(\n",
    "        train_iterations,\n",
    "        np.sum(b),\n",
    "        np.sum(control_bias),\n",
    "    ))\n",
    "    \n",
    "    return fig, err_ax\n",
    "\n",
    "\n",
    "sig_fig, sig_ax = train_2d_sigmoid([ 0.5, 0.9 ], [ 0.5 ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
