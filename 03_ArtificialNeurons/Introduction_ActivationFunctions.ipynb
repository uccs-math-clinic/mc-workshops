{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8b7d20",
   "metadata": {},
   "source": [
    "## Activation Functions and Humanizing Computational Decision Making\n",
    "\n",
    "In this notebook, we direct our attention toward a simple yet profound question which turns out to be quite rich in depth and which ultimately serves at motivation for our discussion with respect to neural networks: \n",
    "\n",
    "_How do we make decisions?_\n",
    "\n",
    "Attempts to answer this question in a meaningful way have yielded rich fields of research in disciplines as diverse as biology, philosophy, computational science, and many others. These results have led to Nobel Prizes, Fields Medals, and influence - if not directly control - many facets of our current life as we know it today. As mathematicians, we will of course choose to ignore many of these results and instead direct our attention toward a drastically oversimplified model which we will in turn incrementally build into something vaguely resembling a working decision-making machine.\n",
    "\n",
    "### Software Prerequisites\n",
    "\n",
    "The following Python libraries are prerequisites to run this notebook; simply run the following code block to install them. They are also listed in the `requirements.txt` file in the root of this notebook's [GitHub repository](https://github.com/uccs-math-clinic/mc-workshops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edab814",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 \\\n",
    "             numpy==1.21.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e807e1",
   "metadata": {},
   "source": [
    "The Python kernel must be restarted after running the above code block for the first time within a particular virtual environment. This may be accomplished by navigating to `Kernel -> Restart` in the menu bar.\n",
    "\n",
    "With our package dependencies installed, we can run the following [boilerplate code](https://en.wikipedia.org/wiki/Boilerplate_code) in order to import the packages needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import time\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f48ad",
   "metadata": {},
   "source": [
    "## Simple Decisions\n",
    "\n",
    "When developing any useful model, it serves one well to examine the simplest possible non-trivial case, characterize that case fully, and incrementally introduce complexity from there. In our case - wherein we are looking for a mathematical formulation of decision-making - we will begin by examining in generality decisions which admit either a \"yes\" response or a \"no\" response. As it turns out, a great many useful decisions can be made by composing such simple yes/no decisions; in fact, classical computing as we know it today is entirely built upon such yes/no (or more accurately \"binary\") logic.\n",
    "\n",
    "To that end, suppose that we have a set of data along with a question for which we hope to answer \"yes\" or \"no\" about. As a very simple example, suppose that we are art museum curators and are tasked with assembling a collection of \"light\" themed paintings. To this end, we might assemble a collection of grayscale paintings and based on the total composition of light paints used in the piece, we may either choose to include it in our collection or not. The decision we must make is whether to include a particular work of art in the collection; in other words, if a painting contains enough light paint, the decision we make is \"yes\" and if it does not, then the answer is \"no\".\n",
    "\n",
    "A function which encodes such logic is called an _activation function_. Activation functions take as input some characteristic of a particular data point (in our example, the amount of \"light\" paint used in a particular piece) and output a particular decision (in our example, whether a particular painting should be included in the themed collection). Perhaps the simplest possible example of such a function might be a step function which outputs `1` (\"yes\") if the input is above a particular threshold $x_0$, and `0` (\"no\") if the function is below that threshold:\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases} \n",
    "      0 & x \\leq x_0 \\\\\n",
    "      1 & x > x_0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Such a function is called a _binary activation_ function and is simply a shifted [Heaviside function](https://en.wikipedia.org/wiki/Heaviside_step_function). Given a particular set of data, such a function would classify that data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b350868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classify(x, x_0):\n",
    "    # We code our classification function manually for pedagogical reasons,\n",
    "    # but point out two equally valid and more elegant ways of performing the\n",
    "    # same task.\n",
    "    \n",
    "    # We create a new list to hold our classification results, which for a particular\n",
    "    # element of x contains 1 for \"yes\" or 0 for \"no\".\n",
    "    #\n",
    "    classification_values = []\n",
    "    \n",
    "    # Iterate through each value in x and add the corresponding classification result\n",
    "    # to the list of classification values.\n",
    "    #\n",
    "    for x_i in x:\n",
    "        if x_i > x_0:\n",
    "            classification_values.append(1)\n",
    "        else:\n",
    "            classification_values.append(0)\n",
    "            \n",
    "            \n",
    "    return classification_values\n",
    "\n",
    "    # The following is the same code, but utilizes numpy's Heaviside function to\n",
    "    # perform all classification in one line of code.\n",
    "    #\n",
    "    #return np.heaviside(x, x_0)\n",
    "    \n",
    "    # The following line is equivalent to the above, but utilizes Python list\n",
    "    # comprehensions to make the classification a bit more elegant.\n",
    "    #\n",
    "    #return [(1 if x_i > x_0 else 0) for x_i in x]\n",
    "    \n",
    "# Now let's try it out! Let's say that we want to output 1 if a particular element is\n",
    "# more than 4, and 0 otherwise.\n",
    "#\n",
    "# We'll test a few values against this.\n",
    "#\n",
    "test_data = [0, 5, 7, 2]\n",
    "print('Decisions for {}: {}'.format(test_data, binary_classify(test_data, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af32eb8e",
   "metadata": {},
   "source": [
    "### Visualizing the Decision\n",
    "\n",
    "To see what a set of decisions might look like with respect to a particular set of input data, we plot the decision function below along with the set of points which represent a decision made for a particular value of $x$. Green dots represent a \"yes\" decision, and red dots represent a \"no\" decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbadc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_endpoints = (-5, 5)\n",
    "binary_center = 3\n",
    "binary_test_values = np.linspace(binary_endpoints[0], binary_endpoints[1], 20) \n",
    "binary_decisions = binary_classify(binary_test_values, binary_center)\n",
    "\n",
    "binary_fig, binary_ax = plt.subplots()\n",
    "\n",
    "# Plot activation function as a step function\n",
    "#\n",
    "binary_ax.step(binary_test_values, binary_decisions)\n",
    "\n",
    "# The next line simply changes the colors based on\n",
    "# the function values.\n",
    "#\n",
    "binary_cmap, binary_norm = mcolors.from_levels_and_colors(\n",
    "    [\n",
    "        binary_endpoints[0],\n",
    "        binary_center,\n",
    "        binary_endpoints[1]\n",
    "    ], \n",
    "    [\n",
    "        'red',\n",
    "        'green'\n",
    "    ]\n",
    ")\n",
    "binary_ax.scatter(binary_test_values, binary_decisions, \n",
    "                  c=binary_test_values, cmap=binary_cmap, norm=binary_norm)\n",
    "binary_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8f6d1",
   "metadata": {},
   "source": [
    "## Less Simple Decisions\n",
    "\n",
    "In the real world, of course, decisions are not always quite so black-and-white. In fact, it's quite rare that any particular decision is met with absolute certainty; it's much more often the case that a binary question is met with responses such as \"probably so\", \"maybe not\", etc. Ideally, we'd like to allow for these possibilities in a somewhat natural way. One way in which we can do this is to \"smooth\" out our activation function - by introducing some values other than zero and one, we may introduce some measure of confidence in a particular decision. If we allow for values _between_ zero and one, we can for instance interpret an output value of `0.70` to mean that we are \"70% confident\" that decision resulting from a particular input should be interpreted as a \"yes\". Furthermore, because we're in the business of trying to fit functions to data, we'd like such a function to be optimizable via our gradient descent algorithm. We therefore would like to find a function which we can tweak in a similar way to the linear model which we trained in [Intro_GradientDescent](../02_GradientDescentLinearRegression/Intro_GradientDescent.ipynb); that is, we'd like a function which has parameters which we can tweak, which is differentiable everywhere, and which provides some measure of \"confidence\" about a decision.\n",
    "\n",
    "As it turns out, several such functions exist, which each have their own set of limitations, advantages, computational requirements, and (in)efficiencies. In this notebook, we will introdude the [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function), which satisfies all of our desired behaviors and is defined thusly:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-wx + b}}\n",
    "$$\n",
    "\n",
    "The values $w$ and $b$ in machine learning literature are often called the _weight_ and _bias_ of the activation function function, respectively. The first of these values scale the sigmoid function, and the second translates it to the left or to the right; these are analogous to the $\\theta_1$ and $\\theta_2$ values which we optimized in our last workshop for linear regression. By adjusting these parameters, we can therefore fit a particular sigmoid function to a set of pre-classified data by way of gradient descent, which we will now proceed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c46edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_classify(data, weight, bias):\n",
    "    # We code our classification function manually for pedagogical reasons,\n",
    "    # but point out two equally valid and more elegant ways of performing the\n",
    "    # same task.\n",
    "    \n",
    "    # We create a new list to hold our classification results, which for a particular\n",
    "    # element of x contains 1 for \"yes\" or 0 for \"no\".\n",
    "    #\n",
    "    classification_values = []\n",
    "    \n",
    "    # Iterate through each value in x and add the corresponding classification result\n",
    "    # to the list of classification values.\n",
    "    #\n",
    "    for x_i in data:\n",
    "        # Calculate sigmoid function value from provided weight and bias terms\n",
    "        #\n",
    "        decision = 1 / (1 + np.exp(-1 * weight * x_i + bias))\n",
    "        classification_values.append(decision)\n",
    "            \n",
    "    return classification_values\n",
    "    \n",
    "# Now let's try it out! Let's say that we want to output 1 if a particular element is\n",
    "# more than 4, and 0 otherwise.\n",
    "#\n",
    "# We'll test a few values against this.\n",
    "#\n",
    "test_data = [0, 5, 7, 2]\n",
    "print('Decisions for {}: {}'.format(test_data, sigmoid_classify(test_data, 1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69a1d1",
   "metadata": {},
   "source": [
    "### Visualizing the Decision\n",
    "\n",
    "We can again visualize what a sigmoid function decides - any value above `0.5` is considered a \"yes\" decision, and below is a \"no\" decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_endpoints = (-5, 5)\n",
    "sigmoid_bias = 2\n",
    "sigmoid_weight = 1\n",
    "sigmoid_test_values = np.linspace(sigmoid_endpoints[0], sigmoid_endpoints[1], 20) \n",
    "sigmoid_decisions = sigmoid_classify(sigmoid_test_values, sigmoid_weight, sigmoid_bias)\n",
    "\n",
    "sigmoid_fig, sigmoid_ax = plt.subplots()\n",
    "\n",
    "# Plot activation function as a step function\n",
    "#\n",
    "sigmoid_ax.plot(sigmoid_test_values, sigmoid_decisions)\n",
    "\n",
    "# The next line simply changes the colors based on\n",
    "# the function values.\n",
    "#\n",
    "sigmoid_cmap, sigmoid_norm = mcolors.from_levels_and_colors(\n",
    "    [\n",
    "        sigmoid_endpoints[0],\n",
    "        sigmoid_bias,\n",
    "        sigmoid_endpoints[1]\n",
    "    ],\n",
    "    [\n",
    "        'red',\n",
    "        'green'\n",
    "    ]\n",
    ")\n",
    "\n",
    "sigmoid_ax.scatter(sigmoid_test_values, np.round(sigmoid_decisions),\n",
    "                   c=sigmoid_test_values, cmap=sigmoid_cmap, norm=sigmoid_norm)\n",
    "\n",
    "sigmoid_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d765a74",
   "metadata": {},
   "source": [
    "## Learning an Activation Function\n",
    "\n",
    "We conclude this notebook by bringing our sigmoid activation function into the context of machine learning by way of this notebook's exercise.\n",
    "\n",
    "Given a particular data set which is labeled with a \"yes\" or \"no\" decision for each data point, we'd like to determine which activation function matches that data set as closely as possible. Our sigmoid activation function contains two parameters (the _weight_ and _bias_ terms) which \n",
    "\n",
    "Recall that for a collection of measured values $x_i$ and model predictions $\\bar{x_i}$, the Mean Squared Error function is given by:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "    L &= \\frac{1}{n}\\sum\\limits_{i=0}^{n}{\\left( \\bar{y_i} - y_i \\right)^2} \\\\\n",
    "      &= \\frac{1}{n}\\sum\\limits_{i=0}^{n}{\\left( \\sigma(x_i) - y_i \\right)^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To find the gradient of $L$ with respect to $w$ and $b$, we must first calculate the partial derivatives $\\frac{\\partial{L}}{\\partial{w}}$ and $\\frac{\\partial{L}}{\\partial{b}}$, and substitute the result of that calculation into the code below to use gradient descent to determine the best-fitting sigmoid function for a particular data set. Try this on your own, and code the results below to see how well our model does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df09f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "fig, (ax, err_ax) = plt.subplots(2, 1)\n",
    "\n",
    "ax.set_title('Predicted Sigmoid')\n",
    "err_ax.set_title('Error (MSE)')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "left_xlim = -5\n",
    "right_xlim = 5\n",
    "\n",
    "known_weight = 1.5\n",
    "known_bias = 2.0\n",
    "\n",
    "x = np.linspace(left_xlim, right_xlim, 50)[:, np.newaxis]\n",
    "y = sigmoid_classify(x, known_weight, known_bias) + np.random.uniform(-0.2, 0.2, x.shape)\n",
    "ax.scatter(x, y)\n",
    "\n",
    "# Initial guess at weight and bias for our optimal sigmoid function\n",
    "w = 4\n",
    "b = 1\n",
    "\n",
    "learning_rate = 1.5\n",
    "\n",
    "\n",
    "def calculate_gradient(weight, bias, x_vals, y_vals):\n",
    "    # Calculate the sigmoid activation function values. These are your predicted values.\n",
    "    #\n",
    "    sigmoid_vals = sigmoid_classify(x_vals, weight, bias)\n",
    "    \n",
    "    # Calculate the difference between the predicted values and the actual data.\n",
    "    #\n",
    "    abs_error = np.subtract(sigmoid_vals, y_vals)\n",
    "   \n",
    "    # This is where you should encode the gradient of your loss function.\n",
    "    #\n",
    "    \n",
    "    # d_weight represents the partial derivative of the loss function with respect to the weight parameter.\n",
    "    #\n",
    "    d_weight = 0\n",
    "    \n",
    "    # d_bias represents the partial derivative of the loss function with respect to the bias parameter.\n",
    "    #\n",
    "    d_bias = 0\n",
    "    \n",
    "    # Calculate mean standard error value\n",
    "    #\n",
    "    mse = np.sum(np.power(abs_error, 2)) / len(x_vals)\n",
    "    \n",
    "    return (d_weight, d_bias, mse)\n",
    "    \n",
    "dw, db, err = calculate_gradient(w, b, x, y)\n",
    "\n",
    "err_vals = [err]\n",
    "y_predicted = sigmoid_classify(x, w, b)\n",
    "z, = ax.plot(x, y_predicted, color='orange')\n",
    "e, = err_ax.plot(np.arange(0, len(err_vals)), err_vals)\n",
    "\n",
    "\n",
    "ax.set_ylim(bottom=-1)\n",
    "err_ax.set_ylim(bottom=0)\n",
    "err_ax.set_xlim(left=0, right=100)\n",
    "\n",
    "convergence_error_threshold = 0.1\n",
    "acceptable_min_gradient = 0.01\n",
    "\n",
    "for i in range(100):        \n",
    "    w = w - (learning_rate * dw)\n",
    "    b = b - (learning_rate * db)\n",
    "    \n",
    "    dw, db, err = calculate_gradient(w, b, x, y)\n",
    "    y_predicted = sigmoid_classify(x, w, b)\n",
    "    err_vals.append(err)\n",
    "    \n",
    "    # Plot new (predicted) sigmoid\n",
    "    #\n",
    "    z.set_ydata(y_predicted)\n",
    "    \n",
    "    # Plot error\n",
    "    #\n",
    "    e.set_xdata(np.arange(0, len(err_vals)))\n",
    "    e.set_ydata(err_vals)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    \n",
    "    # Comment out this line if you want to see how quickly your loss function converges.\n",
    "    time.sleep(0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
